{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ed142-aa88-42fd-be02-f87b5fa5bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple\n",
    "from paddlenlp.datasets import DatasetBuilder\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9260f-aa77-4c26-917e-0a70e7abd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',      # 验证集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    content, label = data\n",
    "                    yield {\"content\": content, \"label\": int(label)}  # 将标签转换为整数\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [0, 1]  # 类别标签为0和1\n",
    "\n",
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n",
    "\n",
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"content\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")  # 标签直接使用0或1\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        preds = paddle.argmax(probs, axis=1).numpy()\n",
    "        labels = labels.numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        correct = metric.compute(probs, paddle.to_tensor(labels))\n",
    "        metric.update(correct)\n",
    "\n",
    "    accu = metric.accumulate()\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"eval loss: {np.mean(losses):.4f}, accu: {accu:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    \n",
    "    return accu, precision, recall, f1\n",
    "\n",
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, batch_size=1):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        results.extend(idx)  # 结果已经是0或1\n",
    "    return results  # 返回预测结果\n",
    "\n",
    "# 定义对数据的预处理函数,处理为模型输入指定list格式\n",
    "def preprocess_prediction_data(data):\n",
    "    examples = []\n",
    "    for content in data:\n",
    "        examples.append({\"content\": content})\n",
    "    return examples\n",
    "\n",
    "# 将list格式的预测结果存储为txt文件，提交格式要求：每行一个类别\n",
    "def write_results(labels, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.writelines(\"\\n\".join(map(str, labels)))  # 确保标签是字符串格式\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置GPU运行\n",
    "    paddle.device.set_device('gpu:0')\n",
    "\n",
    "    df = pd.read_excel(\"social support_coding scheme_0313.xlsx\", sheet_name=1)\n",
    "    df['content'] = df['content'].astype(str)\n",
    "\n",
    "    labels_to_process = ['transp_e', 'emp_e', 'symp_e', 'symptom_i', 'experience_i', 'objective_i']\n",
    "    pretrained_models = ['ernie-3.0-base-zh']\n",
    "    for MODEL_NAME in pretrained_models:\n",
    "        for label in labels_to_process:\n",
    "            print(f\"Processing label: {label}\")\n",
    "            with open(\"best_model_metrics.csv\", \"a\") as f:\n",
    "                f.write(f\"{label}\\n\")\n",
    "            \n",
    "            data = df[['content', label]]\n",
    "            train, dev = train_test_split(data, test_size=0.2, random_state=22)\n",
    "        \n",
    "            # 保存处理后的数据集文件\n",
    "            train.to_csv('train.csv', sep='\\t', encoding = 'utf_8_sig', index=False)\n",
    "            dev.to_csv('dev.csv', sep='\\t', encoding = 'utf_8_sig', index=False)\n",
    "        \n",
    "            model = ppnlp.transformers.AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)  # num_classes为类别数量\n",
    "            tokenizer = ppnlp.transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "            # 加载训练和验证集\n",
    "            train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])\n",
    "        \n",
    "            # 参数设置：\n",
    "            batch_size = 16\n",
    "            max_seq_length = 48\n",
    "            learning_rate = 4e-5\n",
    "            epochs = 4\n",
    "            warmup_proportion = 0.1\n",
    "            weight_decay = 0.0\n",
    "        \n",
    "            # 将数据处理成模型可读入的数据格式\n",
    "            trans_func = partial(\n",
    "                convert_example,\n",
    "                tokenizer=tokenizer,\n",
    "                max_seq_length=max_seq_length)\n",
    "        \n",
    "            batchify_fn = lambda samples, fn=Tuple(\n",
    "                Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "                Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "                Stack()  # labels\n",
    "            ): [data for data in fn(samples)]\n",
    "        \n",
    "            # 训练集迭代器\n",
    "            train_data_loader = create_dataloader(\n",
    "                train_ds,\n",
    "                mode='train',\n",
    "                batch_size=batch_size,\n",
    "                batchify_fn=batchify_fn,\n",
    "                trans_fn=trans_func)\n",
    "        \n",
    "            # 验证集迭代器\n",
    "            dev_data_loader = create_dataloader(\n",
    "                dev_ds,\n",
    "                mode='dev',\n",
    "                batch_size=batch_size,\n",
    "                batchify_fn=batchify_fn,\n",
    "                trans_fn=trans_func)\n",
    "        \n",
    "            num_training_steps = len(train_data_loader) * epochs\n",
    "            lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "        \n",
    "            optimizer = paddle.optimizer.AdamW(\n",
    "                learning_rate=lr_scheduler,\n",
    "                parameters=model.parameters(),\n",
    "                weight_decay=weight_decay,\n",
    "                apply_decay_param_fun=lambda x: x in [\n",
    "                    p.name for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "                ])\n",
    "        \n",
    "            criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "            metric = paddle.metric.Accuracy()              # accuracy评价指标\n",
    "        \n",
    "            # 固定随机种子便于结果的复现\n",
    "            seed = 1024\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            paddle.seed(seed)\n",
    "        \n",
    "            save_dir = \"checkpoint\"\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "        \n",
    "            pre_accu = 0\n",
    "            best_metrics = {}\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                for step, batch in enumerate(train_data_loader, start=1):\n",
    "                    input_ids, token_type_ids, labels = batch\n",
    "                    logits = model(input_ids, token_type_ids)\n",
    "        \n",
    "                    loss = criterion(logits, labels)\n",
    "                    probs = F.softmax(logits, axis=1)\n",
    "                    correct = metric.compute(probs, labels)\n",
    "                    metric.update(correct)\n",
    "                    acc = metric.accumulate()\n",
    "        \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.clear_grad()\n",
    "        \n",
    "                    if step % 10 == 0:\n",
    "                        print(\"epoch: %d, step: %d, loss: %.4f, accu: %.4f\" % (epoch, step, loss, acc))\n",
    "        \n",
    "                accu, precision, recall, f1 = evaluate(model, criterion, metric, dev_data_loader)\n",
    "                print(\"end of epoch:{}, accu:{}\".format(epoch, accu))\n",
    "        \n",
    "                if accu > pre_accu:\n",
    "                    paddle.save(model.state_dict(), os.path.join(save_dir, \"model_state.pdparams\"))\n",
    "                    tokenizer.save_pretrained(save_dir)\n",
    "                    pre_accu = accu\n",
    "                    best_metrics = {\"model_name\": MODEL_NAME, \"accuracy\": accu, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "        \n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "        \n",
    "            with open(\"best_model_metrics.csv\", \"a\") as f:\n",
    "                f.write(f\"model: {best_metrics['model_name']}\\n\")\n",
    "                f.write(f\"Accuracy: {best_metrics['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"Precision: {best_metrics['precision']:.4f}\\n\")\n",
    "                f.write(f\"Recall: {best_metrics['recall']:.4f}\\n\")\n",
    "                f.write(f\"F1 Score: {best_metrics['f1']:.4f}\\n\")\n",
    "            \n",
    "            params_path = 'checkpoint/model_state.pdparams'\n",
    "            if params_path and os.path.isfile(params_path):\n",
    "                # 加载模型参数\n",
    "                state_dict = paddle.load(params_path)\n",
    "                model.set_dict(state_dict)\n",
    "                print(\"Loaded parameters from %s\" % params_path)\n",
    "        \n",
    "            # 测试最优模型参数在验证集上的分数\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\n",
    "        \n",
    "            # # 定义要进行分类的类别\n",
    "            # label_list=list(train[label].unique())\n",
    "            # label_map = { \n",
    "            #     idx: label_text for idx, label_text in enumerate(label_list)\n",
    "            # }\n",
    "            # print(label_map)\n",
    "        \n",
    "            # 读取要进行预测的测试集文件\n",
    "            test = pd.read_csv('./2107_2302_comment_predict.csv')  \n",
    "        \n",
    "            # 对测试集数据进行格式处理\n",
    "            data1 = list(test.content)\n",
    "            \n",
    "            examples = preprocess_prediction_data(data1)\n",
    "        \n",
    "            # 对测试集进行预测\n",
    "            results = predict(model, examples, tokenizer, batch_size=16)\n",
    "            write_results(results, f\"./predict_{label}.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
